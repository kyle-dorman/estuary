{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff249d9-14b6-4f50-a68e-64c29b248ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ba461-1412-4a31-94f6-5d2bd1c62ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "from estuary.model.data import parse_dt_from_pth\n",
    "from estuary.util import broad_band, false_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb06b3-be9d-4a0b-9139-d4c0409a9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(\n",
    "    \"/Users/kyledorman/data/results/estuary/train/20251008-151833/timeseries_preds.csv\"\n",
    ")\n",
    "tdf[\"acquired\"] = tdf.source_tif.apply(lambda a: parse_dt_from_pth(Path(a)))\n",
    "tdf[\"year\"] = tdf.acquired.dt.year\n",
    "tdf[\"y_prob_true\"] = 0.05\n",
    "tdf.loc[tdf.orig_label == \"perched open\", \"y_prob_true\"] = 0.6\n",
    "tdf.loc[tdf.orig_label == \"open\", \"y_prob_true\"] = 0.95\n",
    "tdf = tdf.sort_values(\"acquired\").reset_index(drop=True)\n",
    "print(sorted(tdf.region.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d9258-c253-4ddc-a3ad-2c4960305baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.year == 2024].groupby([\"region\"]).correct.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45555c-298a-4ae6-b2ad-1431d4675770",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.year == 2024].groupby([\"region\"]).correct.mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4549a82-cb07-4429-900b-f00c622c25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf.year == 2024].groupby([\"region\", \"orig_label\"]).correct.mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fd694-32d4-4de5-bb5a-42e02919700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pri_iter():\n",
    "    for region, rrdf in tdf[tdf.region == 84].groupby(\"region\"):\n",
    "        torun = rrdf[\n",
    "            # ((rrdf.orig_label == \"open\") & (rrdf.y_prob < 0.2))\n",
    "            ((rrdf.orig_label == \"closed\") & (rrdf.correct == False))\n",
    "        ]\n",
    "        for _, row in torun.iterrows():\n",
    "            yield region, row.orig_label, row.source_tif, row.y_prob\n",
    "\n",
    "\n",
    "iii = low_pri_iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bf36b-c1d4-430b-956a-bb76ac44f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "region, orig_label, pth, y_prob = next(iii)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "with rasterio.open(pth) as src:\n",
    "    data = src.read(out_dtype=np.float32)\n",
    "    nodata = src.read(1, masked=True).mask\n",
    "    img = false_color(data, nodata)\n",
    "    img = Image.fromarray(img)\n",
    "ax.imshow(img)\n",
    "ax.set_title(f\"{region} - {orig_label} - {Path(pth).stem} - {y_prob:0.3}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3902407-7752-401b-a2a7-2085949b1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = tdf[tdf.region == 2145].copy()\n",
    "rdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce19afa-ce8e-40ec-bb32-07fcb852ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "rdf[rdf.orig_label == \"open\"].plot.scatter(x=\"acquired\", y=\"y_prob\", ax=axes, color=\"green\")\n",
    "rdf[rdf.orig_label == \"closed\"].plot.scatter(x=\"acquired\", y=\"y_prob\", ax=axes, color=\"red\")\n",
    "rdf[rdf.orig_label == \"perched open\"].plot.scatter(x=\"acquired\", y=\"y_prob\", ax=axes, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2f8d7-112b-4bd2-b301-95acdf7e407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_iter(df, count):\n",
    "    print(len(df))\n",
    "    group = []\n",
    "    for _, row in df.iterrows():\n",
    "        group.append((row.source_tif, row.y_prob))\n",
    "        if len(group) == count:\n",
    "            yield group\n",
    "            group = []\n",
    "    if len(group):\n",
    "        yield group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d359d-7680-4e03-aa4e-49ceb686caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = image_iter(rdf[(rdf.orig_label == \"open\") & (rdf.y_prob < 0.2)].sort_values(\"y_prob\"), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89c5f1-74e9-4926-a652-18f1e92b8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(ii)\n",
    "\n",
    "assert len(images)\n",
    "\n",
    "cols = min(len(images), 2)\n",
    "rows = max(1, min(math.ceil(len(images) // 2), 2))\n",
    "fig, axs = plt.subplots(nrows=rows, ncols=cols, figsize=(4 * cols, 4 * rows))\n",
    "\n",
    "if len(images) == 1:\n",
    "    axs = [[axs]]\n",
    "elif rows == 1:\n",
    "    axs = [axs]\n",
    "axs = [ax for axx in axs for ax in axx]\n",
    "for (source_tif, y_prob), ax in zip(images, axs, strict=False):\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    with rasterio.open(source_tif) as src:\n",
    "        data = src.read(out_dtype=np.float32)\n",
    "        nodata = src.read(1, masked=True).mask\n",
    "        if len(data) == 4:\n",
    "            img = false_color(data, nodata)\n",
    "        else:\n",
    "            img = broad_band(data, nodata)\n",
    "        img = Image.fromarray(img)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{y_prob:0.3}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca038ff-158b-4f32-b375-108f43ac736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_changes(\n",
    "    times: pd.Series,\n",
    "    states: np.ndarray,\n",
    ") -> list[tuple[pd.Timestamp, int]]:\n",
    "    \"\"\"Return change timestamps with new-state labels (0 or 1).\n",
    "    Resets across large gaps. Returns a list of\n",
    "    (timestamp, new_state) tuples so callers can distinguish 0→1 vs 1→0.\n",
    "    \"\"\"\n",
    "    t = pd.to_datetime(times).reset_index(drop=True)\n",
    "    dts = t.diff().dt.total_seconds().fillna(0)\n",
    "    changes: list[tuple[pd.Timestamp, pd.Timedelta, int]] = []\n",
    "\n",
    "    prev_state = int(states[0])\n",
    "    prev_time = t.iat[0]\n",
    "    for i in range(1, len(states)):\n",
    "        curr_time = t.iat[i]\n",
    "        s = int(states[i])\n",
    "\n",
    "        # Normal (non-gap) transition detection at the boundary sample\n",
    "        if s != prev_state:\n",
    "            delta = curr_time - prev_time\n",
    "            changes.append((curr_time, delta, s))\n",
    "            prev_state = s\n",
    "            prev_time = curr_time\n",
    "\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7f4a8-5782-4c9c-8c0d-da5e7073d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_smooth(df, time_col, prob_col, days=2):\n",
    "    hours = days * 24 + 4\n",
    "    delta = pd.Timedelta(hours=hours)\n",
    "    g = df[[time_col, prob_col]].copy()\n",
    "    g[time_col] = pd.to_datetime(g[time_col], errors=\"coerce\")\n",
    "    g = g.sort_values(time_col)\n",
    "    new_col = f\"rolling_{prob_col}\"\n",
    "    g[new_col] = g[prob_col].copy()\n",
    "\n",
    "    for idx, row in g.iterrows():\n",
    "        sd = row.acquired - delta\n",
    "        ed = row.acquired + delta\n",
    "        num_before = ((g.acquired > sd) & (g.acquired < row.acquired)).sum()\n",
    "        if not num_before:\n",
    "            continue\n",
    "        cols = g[(g.acquired > sd) & (g.acquired < ed)]\n",
    "        g.loc[idx, new_col] = np.mean(cols[prob_col].to_numpy()).item()\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "rolling = rolling_smooth(rdf, \"acquired\", \"y_prob_true\", days=2)\n",
    "rdf.loc[rolling.index, \"rolling_y_prob_true\"] = rolling[\"rolling_y_prob_true\"].copy()\n",
    "rdf.loc[rdf.index, \"rolling_y_true\"] = (rdf[\"rolling_y_prob_true\"] > 0.5).astype(int).copy()\n",
    "\n",
    "prolling = rolling_smooth(rdf, \"acquired\", \"y_prob\", days=2)\n",
    "rdf.loc[prolling.index, \"rolling_y_prob\"] = prolling[\"rolling_y_prob\"].copy()\n",
    "rdf.loc[rdf.index, \"rolling_y_pred\"] = (rdf[\"rolling_y_prob\"] > 0.5).astype(int).copy()\n",
    "rdf.loc[rdf.index, \"rolling_correct\"] = (rdf[\"rolling_y_pred\"] == rdf[\"y_true\"]).copy()\n",
    "rdf.loc[rdf.index, \"rolling_rolling_correct\"] = (\n",
    "    rdf[\"rolling_y_pred\"] == rdf[\"rolling_y_true\"]\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1968c1c-d5be-4900-b2bc-144e131a1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf[[\"correct\", \"rolling_correct\", \"rolling_rolling_correct\"]].astype(np.int32).mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881c63b-2143-4218-8b86-24ed21a90f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "rdf[rdf.orig_label == \"open\"].plot.scatter(x=\"acquired\", y=\"rolling_y_prob\", ax=axes, color=\"green\")\n",
    "rdf[rdf.orig_label == \"closed\"].plot.scatter(x=\"acquired\", y=\"rolling_y_prob\", ax=axes, color=\"red\")\n",
    "rdf[rdf.orig_label == \"perched open\"].plot.scatter(\n",
    "    x=\"acquired\", y=\"rolling_y_prob\", ax=axes, color=\"blue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4878b-bdd5-4f63-a2de-0de8b59f40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "rdf[rdf.rolling_y_true != rdf.y_true].plot.scatter(\n",
    "    x=\"acquired\", y=\"rolling_y_prob_true\", ax=ax, color=\"purple\"\n",
    ")\n",
    "rdf[rdf.rolling_y_true != rdf.y_true].plot.scatter(\n",
    "    x=\"acquired\", y=\"y_prob_true\", ax=ax, color=\"green\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9899853-a05a-42b1-84dc-e3f5ba7d4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "rdf[rdf.rolling_y_true != rdf.rolling_y_pred].plot.scatter(\n",
    "    x=\"rolling_y_prob\", y=\"y_prob\", ax=ax, color=\"purple\"\n",
    ")\n",
    "rdf[rdf.rolling_y_true == rdf.rolling_y_pred].plot.scatter(\n",
    "    x=\"rolling_y_prob\", y=\"y_prob\", ax=ax, color=\"green\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0acb6f-159a-49a6-a40d-6ad7b5551d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def hmm_decode_irregular(\n",
    "    times: pd.Series,  # datetime-like, sorted\n",
    "    p_open: np.ndarray,  # per-frame probs in [0,1]\n",
    "    tau_open_h: float = 72.0,  # avg dwell time open (hours)\n",
    "    tau_closed_h: float = 72.0,  # avg dwell time closed (hours)\n",
    "    gap_reset_h: float = 72.0,  # break sequence if Δt > gap_reset_h\n",
    "    reliability: float = 1.0,  # 1.0=trust model fully; 0.0=ignore model (0.5)\n",
    "    eps: float = 1e-6,  # numerical floor\n",
    ") -> tuple[np.ndarray, list[tuple[pd.Timestamp, int]]]:\n",
    "    \"\"\"\n",
    "    Two-state HMM (0=closed, 1=open) with time-varying transitions from dwell times.\n",
    "    Viterbi decoding per contiguous segment; gap midpoints generate a change event\n",
    "    if the state after the gap differs from the state before it.\n",
    "\n",
    "    Returns:\n",
    "      states   : np.ndarray[int] of shape (T,)\n",
    "      events   : list of (timestamp, new_state) where state changes\n",
    "    \"\"\"\n",
    "    times = pd.to_datetime(times).reset_index(drop=True)\n",
    "    p_open = np.clip(np.asarray(p_open, dtype=float), eps, 1 - eps)\n",
    "\n",
    "    # Down-weight unreliable sites by mixing with neutral 0.5\n",
    "    if reliability < 1.0:\n",
    "        p_open = reliability * p_open + (1.0 - reliability) * 0.5\n",
    "        p_open = np.clip(p_open, eps, 1 - eps)\n",
    "\n",
    "    p_closed = 1.0 - p_open\n",
    "    loge = np.vstack([np.log(p_closed), np.log(p_open)])  # shape (2, T)\n",
    "\n",
    "    dt_s = times.diff().dt.total_seconds().fillna(0).to_numpy()\n",
    "    dt_h = dt_s / 3600.0\n",
    "\n",
    "    # Identify segment boundaries by large gaps\n",
    "    breaks = np.where(dt_h > gap_reset_h)[0]  # indices i where gap between i-1 and i is large\n",
    "    seg_starts = np.r_[0, breaks + 1]\n",
    "    seg_ends = np.r_[breaks, len(times) - 1]\n",
    "\n",
    "    states = np.zeros(len(times), dtype=np.int8)\n",
    "    events: list[tuple[pd.Timestamp, int]] = []\n",
    "\n",
    "    def _viterbi_segment(s0: int, t_idx: np.ndarray):\n",
    "        \"\"\"\n",
    "        Viterbi on a time slice [t_idx], using time-varying transitions from dt_h.\n",
    "        Assumes dt_h[t] is the delta from t-1->t in hours.\n",
    "        \"\"\"\n",
    "        idx0, idx1 = t_idx[0], t_idx[-1]\n",
    "        Tseg = len(t_idx)\n",
    "\n",
    "        # dp and backpointers\n",
    "        dp = np.full((2, Tseg), -np.inf, dtype=float)\n",
    "        prev = np.full((2, Tseg), -1, dtype=np.int8)\n",
    "\n",
    "        # init with a soft prior favoring s0, but not forcing it (0.8/0.2)\n",
    "        init_prior = np.array([0.8, 0.2]) if s0 == 0 else np.array([0.2, 0.8])\n",
    "        dp[:, 0] = np.log(init_prior + eps) + loge[:, idx0]\n",
    "\n",
    "        for k in range(1, Tseg):\n",
    "            i = t_idx[k]  # global index\n",
    "            # time-varying transition probs from dwell times\n",
    "            p_stay_closed = np.exp(-dt_h[i] / max(tau_closed_h, eps))\n",
    "            p_stay_open = np.exp(-dt_h[i] / max(tau_open_h, eps))\n",
    "            A = np.array(\n",
    "                [\n",
    "                    [p_stay_closed, 1 - p_stay_closed],  # from closed -> [closed, open]\n",
    "                    [1 - p_stay_open, p_stay_open],  # from open   -> [closed, open]\n",
    "                ],\n",
    "                dtype=float,\n",
    "            )\n",
    "            logA = np.log(np.clip(A, eps, 1.0))\n",
    "\n",
    "            # transition to closed (0)\n",
    "            cand0 = np.array([dp[0, k - 1] + logA[0, 0], dp[1, k - 1] + logA[1, 0]])\n",
    "            prev[0, k] = np.argmax(cand0)\n",
    "            dp[0, k] = loge[0, i] + np.max(cand0)\n",
    "            # transition to open (1)\n",
    "            cand1 = np.array([dp[0, k - 1] + logA[0, 1], dp[1, k - 1] + logA[1, 1]])\n",
    "            prev[1, k] = np.argmax(cand1)\n",
    "            dp[1, k] = loge[1, i] + np.max(cand1)\n",
    "\n",
    "        # backtrack\n",
    "        sT = int(np.argmax(dp[:, -1]))\n",
    "        path = np.empty(Tseg, dtype=np.int8)\n",
    "        path[-1] = sT\n",
    "        for k in range(Tseg - 1, 0, -1):\n",
    "            path[k - 1] = prev[path[k], k]\n",
    "        return path\n",
    "\n",
    "    # Decode each contiguous segment, seeding each by the first frame’s argmax\n",
    "    last_state: int | None = None\n",
    "    last_time: pd.Timestamp | None = None\n",
    "\n",
    "    for start, end in zip(seg_starts, seg_ends, strict=False):\n",
    "        seg_idx = np.arange(start, end + 1)\n",
    "        if not len(seg_idx):\n",
    "            continue\n",
    "        seed = int(p_open[start] >= 0.5) if last_state is None else last_state\n",
    "        seg_states = _viterbi_segment(seed, seg_idx)\n",
    "        states[seg_idx] = seg_states\n",
    "\n",
    "        # cross-gap event at segment boundary (midpoint) if state changed across the gap\n",
    "        if last_state is not None and start > 0:\n",
    "            if states[start] != last_state:\n",
    "                delta = times[start] - last_time\n",
    "                events.append((times[start], delta, int(states[start])))\n",
    "\n",
    "        # within-segment events at boundary samples\n",
    "        for k in range(start + 1, end + 1):\n",
    "            if states[k] != states[k - 1]:\n",
    "                delta = times[k] - times[k - 1]\n",
    "                events.append((times[k], delta, int(states[k])))\n",
    "\n",
    "        last_state = int(states[end])\n",
    "        last_time = times[end]\n",
    "\n",
    "    return states, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345d9fc-39a4-454c-9906-4b4b063d596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---- helper: compute events from HMM states (already returned by hmm_decode_irregular,\n",
    "# but we keep this for completeness / unit tests) ----\n",
    "def _events_from_states(times: pd.Series, states: np.ndarray) -> list[tuple[pd.Timestamp, int]]:\n",
    "    times = pd.to_datetime(times).reset_index(drop=True)\n",
    "    ev: list[tuple[pd.Timestamp, int]] = []\n",
    "    for i in range(1, len(states)):\n",
    "        if states[i] != states[i - 1]:\n",
    "            ev.append((times[i], int(states[i])))\n",
    "    return ev\n",
    "\n",
    "\n",
    "# ---- decode + build events for one site ----\n",
    "def _decode_site_to_events(\n",
    "    g: pd.DataFrame,\n",
    "    time_col: str,\n",
    "    prob_col: str,\n",
    "    tau_open_h: float,\n",
    "    tau_closed_h: float,\n",
    "    gap_reset_h: float,\n",
    "    reliability: float,\n",
    ") -> tuple[np.ndarray, list[tuple[pd.Timestamp, int]]]:\n",
    "    states, events = hmm_decode_irregular(\n",
    "        g[time_col],\n",
    "        g[prob_col].to_numpy(),\n",
    "        tau_open_h=tau_open_h,\n",
    "        tau_closed_h=tau_closed_h,\n",
    "        gap_reset_h=gap_reset_h,\n",
    "        reliability=reliability,\n",
    "    )\n",
    "    return states, events  # events: [(timestamp, new_state), ...]\n",
    "\n",
    "\n",
    "# ---- evaluate a single site's params using Hungarian matching ----\n",
    "def _score_site_params(\n",
    "    g: pd.DataFrame,\n",
    "    time_col: str,\n",
    "    prob_col: str,\n",
    "    ytrue_col: str,\n",
    "    tau_open_h: float,\n",
    "    tau_closed_h: float,\n",
    "    gap_reset_h: float,\n",
    "    reliability: float,\n",
    "    tol_hours: float,\n",
    "    smooth_gt: bool = False,\n",
    "    hyst_T_high: float = 0.65,\n",
    "    hyst_T_low: float = 0.45,\n",
    "    hyst_min_run: int = 2,\n",
    "    hyst_gap_reset_h: float | None = None,\n",
    ") -> dict[str, float]:\n",
    "    g = g.sort_values(time_col)\n",
    "    # HMM decode predictions\n",
    "    _, pred_events = _decode_site_to_events(\n",
    "        g, time_col, prob_col, tau_open_h, tau_closed_h, gap_reset_h, reliability\n",
    "    )\n",
    "\n",
    "    # Ground-truth states (optionally smooth with hysteresis to suppress 1-frame glitches)\n",
    "    if smooth_gt:\n",
    "        gt_states = hysteresis_decode(\n",
    "            g[time_col],\n",
    "            g[ytrue_col].astype(float).to_numpy(),  # 0/1 -> float for hysteresis\n",
    "            T_high=hyst_T_high,\n",
    "            T_low=hyst_T_low,\n",
    "            min_run=hyst_min_run,\n",
    "            gap_reset_hours=(hyst_gap_reset_h if hyst_gap_reset_h is not None else gap_reset_h),\n",
    "        )\n",
    "        gt_events = extract_changes(g[time_col], gt_states)\n",
    "    else:\n",
    "        gt_events = extract_changes(g[time_col], g[ytrue_col].to_numpy())\n",
    "\n",
    "    # Hungarian matching (direction required, ± tol_hours)\n",
    "    res = hungarian_match_events_with_tolerance(gt_events, pred_events, max_hours=tol_hours)\n",
    "    tp = len(res[0])\n",
    "    fp = len(res[1])\n",
    "    fn = len(res[2])\n",
    "\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"gt_events\": len(gt_events),\n",
    "        \"pred_events\": len(pred_events),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- LOSO grid search over small param sets ----\n",
    "def evaluate_hmm_loso(\n",
    "    df: pd.DataFrame,\n",
    "    site_col: str = \"region\",\n",
    "    time_col: str = \"acquired\",\n",
    "    prob_col: str = \"y_prob\",\n",
    "    ytrue_col: str = \"y_true\",\n",
    "    param_grid: list[dict] | None = None,\n",
    "    gap_reset_h: float = 72.0,\n",
    "    tol_hours: float = 48.0,\n",
    "    smooth_gt: bool = True,\n",
    "    hyst_T_high: float = 0.65,\n",
    "    hyst_T_low: float = 0.45,\n",
    "    hyst_min_run: int = 2,\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    LOSO over sites. For each held-out site, pick the param set that maximizes mean F1\n",
    "    on the other sites, then evaluate on the held-out.\n",
    "\n",
    "    param_grid: list of dicts with keys in {tau_open_h, tau_closed_h, reliability}.\n",
    "                If None, a small default grid is used.\n",
    "    \"\"\"\n",
    "    if param_grid is None:\n",
    "        param_grid = [\n",
    "            {\"tau_open_h\": 48, \"tau_closed_h\": 48, \"reliability\": 1.0},\n",
    "            {\"tau_open_h\": 72, \"tau_closed_h\": 72, \"reliability\": 1.0},\n",
    "            {\"tau_open_h\": 96, \"tau_closed_h\": 72, \"reliability\": 1.0},\n",
    "            {\"tau_open_h\": 72, \"tau_closed_h\": 96, \"reliability\": 1.0},\n",
    "            {\"tau_open_h\": 72, \"tau_closed_h\": 72, \"reliability\": 0.85},\n",
    "            {\"tau_open_h\": 96, \"tau_closed_h\": 96, \"reliability\": 0.85},\n",
    "        ]\n",
    "\n",
    "    sites = list(df[site_col].unique())\n",
    "    per_site_rows = []\n",
    "    best_params_per_site: dict = {}\n",
    "\n",
    "    for holdout in sites:\n",
    "        dev_sites = [s for s in sites if s != holdout]\n",
    "        df_holdout = df[df[site_col] == holdout]\n",
    "        df_dev = df[df[site_col].isin(dev_sites)]\n",
    "\n",
    "        # grid score on dev\n",
    "        grid_scores = []\n",
    "        for p in param_grid:\n",
    "            f1s = []\n",
    "            for s in dev_sites:\n",
    "                g = df_dev[df_dev[site_col] == s]\n",
    "                m = _score_site_params(\n",
    "                    g,\n",
    "                    time_col,\n",
    "                    prob_col,\n",
    "                    ytrue_col,\n",
    "                    tau_open_h=p[\"tau_open_h\"],\n",
    "                    tau_closed_h=p[\"tau_closed_h\"],\n",
    "                    gap_reset_h=gap_reset_h,\n",
    "                    reliability=p[\"reliability\"],\n",
    "                    tol_hours=tol_hours,\n",
    "                    smooth_gt=smooth_gt,\n",
    "                    hyst_T_high=hyst_T_high,\n",
    "                    hyst_T_low=hyst_T_low,\n",
    "                    hyst_min_run=hyst_min_run,\n",
    "                    hyst_gap_reset_h=gap_reset_h,\n",
    "                )\n",
    "                f1s.append(m[\"f1\"])\n",
    "            grid_scores.append((p, float(np.mean(f1s) if f1s else 0.0)))\n",
    "\n",
    "        # choose best param set by mean dev F1\n",
    "        best_p, _ = max(grid_scores, key=lambda kv: kv[1]) if grid_scores else (param_grid[0], 0.0)\n",
    "        best_params_per_site[holdout] = best_p\n",
    "\n",
    "        # evaluate on holdout with best params\n",
    "        metrics = _score_site_params(\n",
    "            df_holdout,\n",
    "            time_col,\n",
    "            prob_col,\n",
    "            ytrue_col,\n",
    "            tau_open_h=best_p[\"tau_open_h\"],\n",
    "            tau_closed_h=best_p[\"tau_closed_h\"],\n",
    "            gap_reset_h=gap_reset_h,\n",
    "            reliability=best_p[\"reliability\"],\n",
    "            tol_hours=tol_hours,\n",
    "            smooth_gt=smooth_gt,\n",
    "            hyst_T_high=hyst_T_high,\n",
    "            hyst_T_low=hyst_T_low,\n",
    "            hyst_min_run=hyst_min_run,\n",
    "            hyst_gap_reset_h=gap_reset_h,\n",
    "        )\n",
    "        metrics.update(\n",
    "            {\n",
    "                \"site\": holdout,\n",
    "                \"tau_open_h\": best_p[\"tau_open_h\"],\n",
    "                \"tau_closed_h\": best_p[\"tau_closed_h\"],\n",
    "                \"reliability\": best_p[\"reliability\"],\n",
    "            }\n",
    "        )\n",
    "        per_site_rows.append(metrics)\n",
    "\n",
    "    per_site = pd.DataFrame(per_site_rows).sort_values(\"site\")\n",
    "\n",
    "    # micro\n",
    "    tp = per_site[\"tp\"].sum()\n",
    "    fp = per_site[\"fp\"].sum()\n",
    "    fn = per_site[\"fn\"].sum()\n",
    "    micro = {\n",
    "        \"precision_micro\": tp / (tp + fp) if (tp + fp) else 0.0,\n",
    "        \"recall_micro\": tp / (tp + fn) if (tp + fn) else 0.0,\n",
    "        \"f1_micro\": (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) else 0.0,\n",
    "    }\n",
    "    macro = {\n",
    "        \"precision_macro\": per_site[\"precision\"].mean(),\n",
    "        \"recall_macro\": per_site[\"recall\"].mean(),\n",
    "        \"f1_macro\": per_site[\"f1\"].mean(),\n",
    "    }\n",
    "    return per_site, {\"micro\": micro, \"macro\": macro, \"best_params\": best_params_per_site}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cfa3d-b15c-4b02-aa89-7ce404a97e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tdf.sort_values([\"region\", \"acquired\"])  # needs ['region','acquired','y_prob','y_true']\n",
    "\n",
    "per_site, summary = evaluate_hmm_loso(\n",
    "    df,\n",
    "    site_col=\"region\",\n",
    "    time_col=\"acquired\",\n",
    "    prob_col=\"y_prob\",\n",
    "    ytrue_col=\"y_true\",\n",
    "    param_grid=None,  # use the default small grid above\n",
    "    gap_reset_h=75,  # your 2–3 day gap rule\n",
    "    tol_hours=75,  # event matching tolerance\n",
    "    smooth_gt=False,  # smooth GT to suppress 1-frame glitches\n",
    "    hyst_T_high=0.65,\n",
    "    hyst_T_low=0.45,\n",
    "    hyst_min_run=2,\n",
    ")\n",
    "\n",
    "print(per_site)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd572bc0-478e-4dd3-aaba-43be9727d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = tdf.sort_values([\"region\", \"acquired\"]).reset_index(drop=True)\n",
    "for region in tdf.region.unique():\n",
    "    g = tdf[tdf[\"region\"] == region]\n",
    "    changes = extract_changes(g.acquired, g.y_true.to_numpy())\n",
    "\n",
    "    events = extract_changes(g.acquired, g.y_pred.to_numpy())\n",
    "\n",
    "    res = hungarian_match_events_with_tolerance(changes, events)\n",
    "    tp = len(res[0])\n",
    "    fp = len(res[1])\n",
    "    fn = len(res[2])\n",
    "\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "\n",
    "    print(\n",
    "        region,\n",
    "        round(\n",
    "            {\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"f1\": f1,\n",
    "            }[\"f1\"],\n",
    "            3,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6ab7d-9815-4d56-8c9d-86da68ed1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2bce9-a0d3-450c-9dfd-d0024ee7b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = extract_changes(rdf.acquired, rdf.y_true.to_numpy())\n",
    "print(len(changes), len([c for c in changes if c[1] > pd.Timedelta(days=5)]))\n",
    "\n",
    "changes2 = extract_changes(rdf.acquired, rdf.rolling_y_true.to_numpy())\n",
    "len(changes2), len([c for c in changes2 if c[1] > pd.Timedelta(days=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a711d-ae37-46c5-979a-8553daa50810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add near the imports\n",
    "import math\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def hungarian_match_events_with_tolerance(\n",
    "    gt: list[tuple[pd.Timestamp, pd.Timedelta, int]],\n",
    "    pr: list[tuple[pd.Timestamp, pd.Timedelta, int]],\n",
    "    max_hours: float = 52.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimal 1-to-1 matching of change events using the Hungarian algorithm.\n",
    "\n",
    "    Inputs (timestamp, delta, new_state) tuples.\n",
    "    - Pairs farther than `max_hours` are disallowed.\n",
    "\n",
    "    Cost = absolute time difference in HOURS. We minimize total cost.\n",
    "\n",
    "    Returns:\n",
    "        list tp pairs, fps, fns.\n",
    "    \"\"\"\n",
    "    G, P = len(gt), len(pr)\n",
    "    if G == 0 and P == 0:\n",
    "        return [], [], []\n",
    "    if G == 0:\n",
    "        return [], pr, []\n",
    "    if P == 0:\n",
    "        return [], [], gt\n",
    "\n",
    "    # --- build cost matrix (G x P) in HOURS ---\n",
    "    BIG = 1e12  # \"infinite\" cost to forbid a pairing\n",
    "    cost = np.full((G, P), BIG, dtype=float)\n",
    "\n",
    "    for i, (gt_t, _, gt_s) in enumerate(gt):\n",
    "        for j, (pr_t, _, pr_s) in enumerate(pr):\n",
    "            # direction constraint\n",
    "            if gt_s != pr_s:\n",
    "                continue  # leave as BIG (forbidden)\n",
    "            # time window constraint\n",
    "            dhours = abs((pr_t - gt_t).total_seconds()) / 3600.0\n",
    "            if dhours <= max_hours:\n",
    "                cost[i, j] = dhours  # smaller is better\n",
    "\n",
    "    # --- Hungarian solve ---\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "    unused_g = [True] * G\n",
    "    unused_p = [True] * P\n",
    "\n",
    "    tp = []\n",
    "    # collect feasible matches (cost < BIG)\n",
    "    pairs: list[tuple[pd.Timestamp, pd.Timestamp, pd.Timedelta]] = []\n",
    "    for r, c in zip(row_ind, col_ind, strict=False):\n",
    "        if cost[r, c] >= BIG:  # forbidden assignments are ignored\n",
    "            continue\n",
    "        gt_t = gt[r]\n",
    "        pr_t = pr[c]\n",
    "        tp.append((gt_t, pr_t))\n",
    "        unused_g[r] = False\n",
    "        unused_p[c] = False\n",
    "    fp = [a for unused, a in zip(unused_p, pr, strict=False) if unused]\n",
    "    fn = [a for unused, a in zip(unused_g, gt, strict=False) if unused]\n",
    "\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "tp, fp, fn = hungarian_match_events_with_tolerance(changes, changes2)\n",
    "\n",
    "len(tp), len(fp), len(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d41aa-06c6-4505-903c-30f508b8f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iii = iter(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0bd7f-0925-4dde-9ff7-884bd1c45721",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, td, s = next(iii)\n",
    "\n",
    "print(td)\n",
    "print(s)\n",
    "\n",
    "delta = pd.Timedelta(hours=4 * 24 + 3)\n",
    "start = ts - delta\n",
    "end = ts + delta\n",
    "cols = rdf[(rdf.acquired > start) & (rdf.acquired < end)]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(cols), figsize=(5 * len(cols), 5))\n",
    "\n",
    "if len(cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ii, ax in zip(range(len(cols)), axes, strict=False):\n",
    "    row = cols.iloc[ii]\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    with rasterio.open(row.source_tif) as src:\n",
    "        data = src.read(out_dtype=np.float32)\n",
    "        nodata = src.read(1, masked=True).mask\n",
    "        if len(data) == 4:\n",
    "            img = false_color(data, nodata)\n",
    "        else:\n",
    "            img = broad_band(data, nodata)\n",
    "        img = Image.fromarray(img)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(row.orig_label + \" \" + str(round(row.rolling_y_prob_true, 3)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fdbad-7c69-4f0a-ab46-73d521eef28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Each input is a list of (timestamp, state_int) pairs\n",
    "#   gt_tp  : ground-truth events that were matched (TP)\n",
    "#   gt_fn  : ground-truth events that were missed (FN)\n",
    "#   pr_tp  : predicted events that were matched (TP)\n",
    "#   pr_fp  : predicted events that were extra (FP)\n",
    "def plot_change_events_timeline(\n",
    "    gt_tp: list[tuple[pd.Timestamp, int]],\n",
    "    gt_fn: list[tuple[pd.Timestamp, int]],\n",
    "    pr_tp: list[tuple[pd.Timestamp, int]],\n",
    "    pr_fp: list[tuple[pd.Timestamp, int]],\n",
    "    title: str = \"Change Events Timeline (0=closed, 1=open)\",\n",
    "    ymin: float = -0.2,\n",
    "    ymax: float = 1.2,\n",
    "    gt_offset: float = +0.04,  # vertical offset so GT and PRED don't overlap exactly\n",
    "    pr_offset: float = -0.04,\n",
    "    figsize=(11, 3.8),\n",
    "):\n",
    "    def _split_xy(events, offset=0.0):\n",
    "        if not events:\n",
    "            return [], []\n",
    "        xs = [pd.to_datetime(t) for (t, _, s) in events]\n",
    "        ys = [int(s) + offset for (t, _, s) in events]\n",
    "        return xs, ys\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Base guide lines for states\n",
    "    ax.axhline(0, color=\"0.85\", lw=1)\n",
    "    ax.axhline(1, color=\"0.85\", lw=1)\n",
    "\n",
    "    # Unpack to X/Y\n",
    "    gt_tp_x, gt_tp_y = _split_xy(gt_tp, gt_offset)\n",
    "    gt_fn_x, gt_fn_y = _split_xy(gt_fn, gt_offset)\n",
    "    pr_tp_x, pr_tp_y = _split_xy(pr_tp, pr_offset)\n",
    "    pr_fp_x, pr_fp_y = _split_xy(pr_fp, pr_offset)\n",
    "\n",
    "    # Plot\n",
    "    # GT events: triangles\n",
    "    if gt_tp_x:\n",
    "        ax.scatter(gt_tp_x, gt_tp_y, marker=\"^\", s=60, color=\"green\", label=\"GT TP\")\n",
    "    if gt_fn_x:\n",
    "        ax.scatter(\n",
    "            gt_fn_x,\n",
    "            gt_fn_y,\n",
    "            marker=\"^\",\n",
    "            s=60,\n",
    "            color=\"yellow\",\n",
    "            edgecolor=\"k\",\n",
    "            linewidth=0.5,\n",
    "            label=\"GT FN\",\n",
    "        )\n",
    "\n",
    "    # PRED events: circles\n",
    "    if pr_tp_x:\n",
    "        ax.scatter(pr_tp_x, pr_tp_y, marker=\"o\", s=50, color=\"green\", alpha=0.9, label=\"Pred TP\")\n",
    "    if pr_fp_x:\n",
    "        ax.scatter(pr_fp_x, pr_fp_y, marker=\"o\", s=50, color=\"red\", alpha=0.9, label=\"Pred FP\")\n",
    "\n",
    "    # Axis cosmetics\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels([\"closed (0)\", \"open (1)\"])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"State\")\n",
    "\n",
    "    # Nice date ticks\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "\n",
    "    # # Build legend only for shown handles\n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # if handles:\n",
    "    #     ax.legend(loc=\"middle left\", ncols=2, frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_change_events_timeline([a[0] for a in tp], fn, [a[1] for a in tp], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1c7f7-bc43-422c-991c-2f6416d6371e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2ec24-c202-4337-ac41-9047ac93bad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
