{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75a683-769b-4733-a4bd-c7ee0e8975bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e40ba3-9fe7-4de3-a194-ff0990b21190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dc35d517ff4a8daea111dd701a8af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bbc578223a4d089669579723274e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49001eec694040a5b904e60c3d3a945d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e51d05f615457db34437487523ca33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"image-feature-extraction\", model=\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df13fdc-863a-474e-a5ae-a316f51c3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import load_image\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = load_image(url)\n",
    "\n",
    "features = pipe(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32fc52f0-d98c-411a-8ec4-190b55a18d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c8aa7b5-8133-4e58-b568-49b201c8325c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6992dd89cd9c40f5827709c1c064a227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/kyledorman/.cache/huggingface/hub/models--facebook--dinov3-convnext-tiny-pretrain-lvd1689m/snapshots/10d30274b4d445111e2d5bf75ac93bbd94db274b/preprocessor_config.json\n",
      "Image processor DINOv3ViTImageProcessorFast {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"disable_grouping\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"DINOv3ViTImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_tensors\": null,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/kyledorman/.cache/huggingface/hub/models--facebook--dinov3-convnext-tiny-pretrain-lvd1689m/snapshots/10d30274b4d445111e2d5bf75ac93bbd94db274b/config.json\n",
      "Model config DINOv3ConvNextConfig {\n",
      "  \"architectures\": [\n",
      "    \"DINOv3ConvNextModel\"\n",
      "  ],\n",
      "  \"depths\": [\n",
      "    3,\n",
      "    3,\n",
      "    9,\n",
      "    3\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.0,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_sizes\": [\n",
      "    96,\n",
      "    192,\n",
      "    384,\n",
      "    768\n",
      "  ],\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"layer_scale_init_value\": 1e-06,\n",
      "  \"model_type\": \"dinov3_convnext\",\n",
      "  \"num_channels\": 3,\n",
      "  \"transformers_version\": \"4.56.0\"\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/kyledorman/.cache/huggingface/hub/models--facebook--dinov3-convnext-tiny-pretrain-lvd1689m/snapshots/10d30274b4d445111e2d5bf75ac93bbd94db274b/model.safetensors\n",
      "All model checkpoint weights were used when initializing DINOv3ConvNextModel.\n",
      "\n",
      "All the weights of DINOv3ConvNextModel were initialized from the model checkpoint at facebook/dinov3-convnext-tiny-pretrain-lvd1689m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DINOv3ConvNextModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "pretrained_model_name = \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name,\n",
    ")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "pooled_output = outputs.pooler_output\n",
    "print(\"Pooled output shape:\", pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb108a35-71e1-468c-a9ef-b3932e14e22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
