{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e32b2f-2b1b-4a60-990d-710382d94f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503e454-e0ec-4acd-9cb8-d07c651be24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from estuary.model.data import parse_dt_from_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc4f89-506a-4a47-a3d5-e5a3bd4ac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_regions = pd.read_csv(\"/Volumes/x10pro/estuary/geos/skipped_regions.csv\")[\n",
    "    \"Site code\"\n",
    "].to_list()\n",
    "\n",
    "gdf = gpd.read_file(\"/Users/kyledorman/data/estuary/geos/ca_data_w_usgs.geojson\")\n",
    "gdf = gdf[~gdf[\"Site code\"].isin(skipped_regions)].copy()\n",
    "gdf = gdf.set_index(\"Site code\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2011c-333c-4b44-b44c-db74501ada8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/kyledorman/data/estuary/geos/ca_empa_matching_sites.json\") as f:\n",
    "    matching_sites = json.load(f)\n",
    "\n",
    "matching_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49867bc-383e-4eea-a8a9-cf886dc67f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "empa = pl.read_csv(\"/Volumes/x10pro/estuary/ca_all/empa/logger-raw-publish.csv\")\n",
    "# define time parsing (try multiple formats)\n",
    "parsed_dt = pl.coalesce(\n",
    "    [\n",
    "        pl.col(\"samplecollectiontimestamp\").str.strptime(\n",
    "            pl.Datetime, \"%d/%m/%Y %H:%M:%S\", strict=False\n",
    "        ),\n",
    "        pl.col(\"samplecollectiontimestamp\").str.strptime(\n",
    "            pl.Datetime, \"%d/%m/%Y %H:%M:%S%.f\", strict=False\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# offsets relative to UTC (Polars doesn’t know “PST/PDT” by name)\n",
    "# PST = UTC−8, PDT = UTC−7\n",
    "empa = empa.with_columns([parsed_dt.alias(\"samplecollectiontimestamp_parsed\")])\n",
    "\n",
    "# apply offset based on timezone\n",
    "empa = empa.with_columns(\n",
    "    [\n",
    "        pl.when(pl.col(\"samplecollectiontimezone\") == \"PST\")\n",
    "        .then(pl.col(\"samplecollectiontimestamp_parsed\") + pl.duration(hours=8))  # PST -> UTC\n",
    "        .when(pl.col(\"samplecollectiontimezone\") == \"PDT\")\n",
    "        .then(pl.col(\"samplecollectiontimestamp_parsed\") + pl.duration(hours=7))  # PDT -> UTC\n",
    "        .when(pl.col(\"samplecollectiontimezone\") == \"UTC\")\n",
    "        .then(pl.col(\"samplecollectiontimestamp_parsed\"))\n",
    "        .otherwise(pl.col(\"samplecollectiontimestamp_parsed\"))\n",
    "        .alias(\"samplecollectiontimestamp_utc2\")\n",
    "    ]\n",
    ")\n",
    "empa = empa.filter(pl.col(\"siteid\").is_in(list(matching_sites.values())))\n",
    "empa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b74f41-00a1-47c2-a5dc-316ea2c22a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ids = empa[\"siteid\"].unique().sort()\n",
    "\n",
    "for si in site_ids:\n",
    "    region = next(k for k, v in matching_sites.items() if v == si)\n",
    "    name = gdf.loc[int(region)][\"Site name\"]\n",
    "    print(si, region, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b56385-407b-42c4-a966-2997cd7b4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (\n",
    "    empa.group_by([\"siteid\", \"sensorid\"])\n",
    "    .agg(pl.len().alias(\"n_samples\"))\n",
    "    .sort([\"siteid\", \"sensorid\"])\n",
    ")\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385d8fe-dc92-4309-ae5f-216e48baa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = (\n",
    "    empa.group_by([\"siteid\", \"sensorid\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"samplecollectiontimestamp_parsed\").min().alias(\"start\"),\n",
    "            pl.col(\"samplecollectiontimestamp_parsed\").max().alias(\"end\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort([\"siteid\", \"sensorid\"])\n",
    ")\n",
    "print(ranges)\n",
    "\n",
    "pdf = ranges.to_pandas()\n",
    "\n",
    "# Combine siteid & sensorid as label\n",
    "pdf[\"label\"] = pdf[\"siteid\"] + \" - \" + pdf[\"sensorid\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "for i, row in pdf.iterrows():\n",
    "    ax.plot([row[\"start\"], row[\"end\"]], [i, i], lw=6, label=row[\"label\"])\n",
    "\n",
    "ax.set_yticks(range(len(pdf)))\n",
    "ax.set_yticklabels(pdf[\"label\"])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(\"Sensor Availability Timeline\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915eb0a9-9b18-4517-83c3-83ecfb61dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pl.read_csv(\"/Volumes/x10pro/estuary/ca_all/empa/logger-raw-depth-correction-publish.csv\")\n",
    "corr = corr.with_columns(\n",
    "    [\n",
    "        pl.coalesce(\n",
    "            [\n",
    "                pl.col(\"samplecollectiontimestamp\").str.strptime(\n",
    "                    pl.Datetime, \"%d/%m/%Y %H:%M:%S\", strict=False\n",
    "                ),\n",
    "                pl.col(\"samplecollectiontimestamp\").str.strptime(\n",
    "                    pl.Datetime, \"%d/%m/%Y %H:%M:%S%.f\", strict=False\n",
    "                ),\n",
    "            ]\n",
    "        ).alias(\"samplecollectiontimestamp_parsed\")\n",
    "    ]\n",
    ")\n",
    "corr = corr.filter(pl.col(\"siteid\").is_in(list(matching_sites.values())))\n",
    "\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f47ab-69c7-4e29-982e-5bec83068f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = (\n",
    "    corr.group_by([\"siteid\", \"sensorid\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"samplecollectiontimestamp_parsed\").min().alias(\"start\"),\n",
    "            pl.col(\"samplecollectiontimestamp_parsed\").max().alias(\"end\"),\n",
    "        ]\n",
    "    )\n",
    "    # Filter groups where (end - start) > 90 days\n",
    "    .filter((pl.col(\"end\") - pl.col(\"start\")) > pl.duration(days=90))\n",
    "    .sort([\"siteid\", \"sensorid\"])\n",
    ")\n",
    "# print(ranges)\n",
    "\n",
    "pdf = ranges.to_pandas()\n",
    "pdf = pdf.sort_values(by=\"siteid\")\n",
    "\n",
    "# Combine siteid & sensorid as label\n",
    "pdf[\"label\"] = pdf[\"siteid\"] + \" - \" + pdf[\"sensorid\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "for i, row in pdf.iterrows():\n",
    "    ax.plot([row[\"start\"], row[\"end\"]], [i, i], lw=6, label=row[\"label\"])\n",
    "\n",
    "ax.set_yticks(range(len(pdf)))\n",
    "ax.set_yticklabels(pdf[\"label\"])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(\"Sensor Availability Timeline\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85120677-e63c-4206-989a-a9a46e08b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = pd.read_csv(Path(\"/Volumes/x10pro/estuary/ca_all/\") / \"preds.csv\")\n",
    "preds_all[\"acquired\"] = preds_all[\"source_tif\"].apply(lambda p: parse_dt_from_pth(Path(p)))\n",
    "preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4011d7-5df2-4c76-80db-a4b2ccafab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = empa.filter((pl.col(\"siteid\") == \"SC-VEN\") & (pl.col(\"sensorid\") == \"X1683\"))[\n",
    "    [\"samplecollectiontimestamp_utc2\", \"raw_h2otemp\", \"raw_conductivity\"]\n",
    "].to_pandas()\n",
    "a[\"raw_h2otemp\"] = a[\"raw_h2otemp\"].apply(pd.to_numeric)\n",
    "a[\"raw_conductivity\"] = a[\"raw_conductivity\"].apply(pd.to_numeric)\n",
    "a = a.rename(columns={\"samplecollectiontimestamp_utc2\": \"acquired\"})\n",
    "\n",
    "b = corr.filter((pl.col(\"siteid\") == \"SC-VEN\") & (pl.col(\"sensorid\") == \"X1683\"))[\n",
    "    [\"samplecollectiontimestamp_parsed\", \"corrected_depth\"]\n",
    "].to_pandas()\n",
    "b[\"corrected_depth\"] = b[\"corrected_depth\"].apply(pd.to_numeric)\n",
    "\n",
    "b = b.rename(columns={\"samplecollectiontimestamp_parsed\": \"acquired\"})\n",
    "\n",
    "pd.merge(a, b, on=\"acquired\", how=\"outer\").raw_conductivity.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d06164-0da9-4f58-bed2-863d8baf623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(a, b, on=\"acquired\", how=\"outer\").isna().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3f434-d561-4d16-8392-f385262c8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_gaps(\n",
    "    df_time_sorted,\n",
    "    time_col=\"acquired\",\n",
    "    min_gap=pd.Timedelta(days=2.5),\n",
    "    edge_buffer=pd.Timedelta(hours=6),\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a list of (gap_start, gap_end) where time delta between consecutive\n",
    "    samples exceeds `min_gap`. Each interval is trimmed by `edge_buffer` on both ends.\n",
    "    \"\"\"\n",
    "    t = df_time_sorted[time_col].values\n",
    "    if len(t) < 2:\n",
    "        return []\n",
    "\n",
    "    # compute diffs\n",
    "    ts = df_time_sorted[time_col].reset_index(drop=True)\n",
    "    d = ts.diff()\n",
    "\n",
    "    gaps = []\n",
    "    for i in range(1, len(ts)):\n",
    "        if d.iloc[i] > min_gap:\n",
    "            start_raw = ts.iloc[i - 1]\n",
    "            end_raw = ts.iloc[i]\n",
    "\n",
    "            # dynamic safety: don’t over-trim for short-but-qualifying gaps\n",
    "            # use the smaller of requested buffer and 10% of gap\n",
    "            dyn_buf = min(edge_buffer, (end_raw - start_raw) / 10)\n",
    "\n",
    "            start = start_raw + dyn_buf\n",
    "            end = end_raw - dyn_buf\n",
    "            if start < end:\n",
    "                gaps.append((start, end))\n",
    "    return gaps\n",
    "\n",
    "\n",
    "def plot_metric(sdf, col, site, save=False):\n",
    "    # preds_all: ['acquired','y_true','y_prob','y_pred','region']\n",
    "    # sdf:       ['acquired', col]\n",
    "    preds_all_plot = preds_all[preds_all.region == site].copy()\n",
    "    sdf_plot = sdf.copy()\n",
    "\n",
    "    # normalize times\n",
    "    preds_all_plot[\"acquired\"] = (\n",
    "        pd.to_datetime(preds_all_plot[\"acquired\"], errors=\"coerce\", utc=True)\n",
    "        .dt.tz_convert(\"UTC\")\n",
    "        .dt.tz_localize(None)\n",
    "        .astype(\"datetime64[ns]\")\n",
    "    )\n",
    "    sdf_plot[\"acquired\"] = (\n",
    "        pd.to_datetime(sdf_plot[\"acquired\"], errors=\"coerce\", utc=True)\n",
    "        .dt.tz_convert(\"UTC\")\n",
    "        .dt.tz_localize(None)\n",
    "        .astype(\"datetime64[ns]\")\n",
    "    )\n",
    "    preds_all_plot = preds_all_plot.sort_values(\"acquired\").dropna(subset=[\"acquired\"])\n",
    "    sdf_plot = (\n",
    "        sdf_plot.sort_values(\"acquired\")\n",
    "        .dropna(subset=[\"acquired\"])\n",
    "        .drop_duplicates(subset=\"acquired\", keep=\"first\")\n",
    "    )\n",
    "\n",
    "    # keep preds within depth time span\n",
    "    if not sdf_plot.empty:\n",
    "        preds_all_plot = preds_all_plot[\n",
    "            (preds_all_plot.acquired >= sdf_plot.acquired.min())\n",
    "            & (preds_all_plot.acquired <= sdf_plot.acquired.max())\n",
    "        ]\n",
    "\n",
    "    def find_state_changes(df, state_col, include_first=True):\n",
    "        s = df[state_col].astype(\"int64\")\n",
    "        changed = s.ne(s.shift(1))\n",
    "        if not include_first and len(changed):\n",
    "            changed.iloc[0] = False\n",
    "        return df.loc[changed, [\"acquired\", state_col]].rename(columns={state_col: \"new_state\"})\n",
    "\n",
    "    # change points\n",
    "    true_changes = find_state_changes(preds_all_plot, \"y_true\", include_first=True)\n",
    "    pred_changes = find_state_changes(preds_all_plot, \"y_pred\", include_first=True)\n",
    "\n",
    "    # nearest join to grab metric value at change times\n",
    "    true_cp = pd.merge_asof(\n",
    "        true_changes.sort_values(\"acquired\"),\n",
    "        sdf_plot.sort_values(\"acquired\"),\n",
    "        on=\"acquired\",\n",
    "        direction=\"nearest\",\n",
    "    )\n",
    "    pred_cp = pd.merge_asof(\n",
    "        pred_changes.sort_values(\"acquired\"),\n",
    "        sdf_plot.sort_values(\"acquired\"),\n",
    "        on=\"acquired\",\n",
    "        direction=\"nearest\",\n",
    "    )\n",
    "\n",
    "    # --- compute data gaps from the metric series once; reuse for both plots ---\n",
    "    gaps = _find_gaps(\n",
    "        preds_all_plot[[\"acquired\"]].sort_values(\"acquired\"),\n",
    "        time_col=\"acquired\",\n",
    "        min_gap=pd.Timedelta(days=4.5),\n",
    "        edge_buffer=pd.Timedelta(hours=12),\n",
    "    )  # tweak as you like\n",
    "\n",
    "    def format_time_axis(ax):\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "\n",
    "    # -------- Plot 1: labels --------\n",
    "    fig1, ax1 = plt.subplots(figsize=(11, 4))\n",
    "    ax1.plot(sdf_plot[\"acquired\"], sdf_plot[col], lw=1.5, color=\"k\")\n",
    "    ax1.set_title(f\"{col} with LABEL State Changes (y_true)\")\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_ylabel(col)\n",
    "\n",
    "    # blue filled bands for gaps (solid, not dashed)\n",
    "    for start, end in gaps:\n",
    "        ax1.axvspan(start, end, color=\"blue\", alpha=0.15, linewidth=0)\n",
    "\n",
    "    for _, row in true_cp.iterrows():\n",
    "        color = \"green\" if int(row[\"new_state\"]) == 1 else \"red\"\n",
    "        ax1.axvline(row[\"acquired\"], linestyle=\"--\", color=color, alpha=0.8, linewidth=1.25)\n",
    "        ax1.scatter(row[\"acquired\"], row[col], color=color, s=35, zorder=3)\n",
    "\n",
    "    format_time_axis(ax1)\n",
    "    fig1.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(f\"/Users/kyledorman/data/estuary/display/malibu_{col}_label.png\", dpi=200)\n",
    "\n",
    "    # -------- Plot 2: predictions --------\n",
    "    fig2, ax2 = plt.subplots(figsize=(11, 4))\n",
    "    ax2.plot(sdf_plot[\"acquired\"], sdf_plot[col], lw=1.5, color=\"k\")\n",
    "    ax2.set_title(f\"{col} with PREDICTED State Changes (y_pred)\")\n",
    "    ax2.set_xlabel(\"Time\")\n",
    "    ax2.set_ylabel(col)\n",
    "\n",
    "    for start, end in gaps:\n",
    "        ax2.axvspan(start, end, color=\"blue\", alpha=0.15, linewidth=0)\n",
    "\n",
    "    for _, row in pred_cp.iterrows():\n",
    "        color = \"green\" if int(row[\"new_state\"]) == 1 else \"red\"\n",
    "        ax2.axvline(row[\"acquired\"], linestyle=\"--\", color=color, alpha=0.8, linewidth=1.25)\n",
    "        ax2.scatter(row[\"acquired\"], row[col], color=color, s=35, zorder=3)\n",
    "\n",
    "    format_time_axis(ax2)\n",
    "    fig2.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(f\"/Users/kyledorman/data/estuary/display/malibu_{col}_pred.png\", dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf2961-958f-4030-811d-8f8219603802",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = (\n",
    "    empa.filter((pl.col(\"siteid\") == \"SC-VEN\") & (pl.col(\"sensorid\") == \"X1683\"))[\n",
    "        [\n",
    "            \"raw_depth\",\n",
    "            \"raw_pressure\",\n",
    "            \"raw_h2otemp\",\n",
    "            \"raw_ph\",\n",
    "            \"raw_conductivity\",\n",
    "            \"raw_turbidity\",\n",
    "            \"raw_do\",\n",
    "            \"raw_salinity\",\n",
    "            \"sensorid\",\n",
    "            \"sensortype\",\n",
    "            \"samplecollectiontimestamp_parsed\",\n",
    "        ]\n",
    "    ]\n",
    "    .to_pandas()\n",
    "    .rename(columns={\"samplecollectiontimestamp_parsed\": \"acquired\"})\n",
    ")\n",
    "\n",
    "# Ensure the index is datetime, not a column\n",
    "aaa[\"acquired\"] = pd.to_datetime(aaa.acquired)\n",
    "aaa = aaa.sort_values([\"acquired\"])\n",
    "\n",
    "# aaa = aaa[aaa[\"acquired\"] > pd.Timestamp(\"2021-09-15\")].copy()\n",
    "\n",
    "aaa[\"raw_depth\"] = aaa[\"raw_depth\"].apply(pd.to_numeric)\n",
    "aaa[\"raw_pressure\"] = aaa[\"raw_pressure\"].apply(pd.to_numeric)\n",
    "aaa[\"raw_h2otemp\"] = aaa[\"raw_h2otemp\"].apply(pd.to_numeric)\n",
    "aaa[\"raw_ph\"] = aaa[\"raw_ph\"].apply(pd.to_numeric).clip(6, 100)\n",
    "aaa[\"raw_conductivity\"] = aaa[\"raw_conductivity\"].apply(pd.to_numeric)\n",
    "aaa[\"raw_turbidity\"] = aaa[\"raw_turbidity\"].apply(pd.to_numeric)\n",
    "aaa[\"raw_do\"] = aaa[\"raw_do\"].apply(pd.to_numeric)\n",
    "aaa[\"raw_salinity\"] = aaa[\"raw_salinity\"].apply(pd.to_numeric)\n",
    "\n",
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecd813-9129-41a1-a2d3-eb59bead7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(aaa, \"raw_pressure\", 21, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f11f2-cbe8-40fa-a4cd-d18d206dcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = counts.filter(pl.col(\"siteid\") == \"SC-VEN\").to_pandas()\n",
    "cc.sort_values(\"n_samples\").head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f1a88-5bb6-406b-8dda-2d80b1d96be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
